#!/usr/bin/env python3
"""
ğŸ¯ MLflow Integration - Quick Start Guide & Test
"""

import os
import sys
import json
import requests
from datetime import datetime

def show_status():
    """Show current MLflow integration status"""
    print("ğŸ¯ MLflow Integration Status")
    print("="*30)
    
    # Check MLflow data
    if os.path.exists("mlruns_local"):
        print("âœ… MLflow local storage: READY")
        print(f"ğŸ“ Location: {os.path.abspath('mlruns_local')}")
    else:
        print("âŒ MLflow local storage: NOT FOUND")
    
    # Check integration file
    if os.path.exists("mlflow_integration.py"):
        print("âœ… MLflow integration: READY")
    else:
        print("âŒ MLflow integration: NOT FOUND")
    
    # Check API
    try:
        response = requests.get("http://localhost:5001/health", timeout=3)
        if response.status_code == 200:
            print("âœ… API server: RUNNING")
        else:
            print("âŒ API server: ERROR")
    except:
        print("âŒ API server: NOT RUNNING")

def test_prediction_with_logging():
    """Test making a prediction and verify MLflow logging"""
    print("\\nğŸš€ Testing Prediction + MLflow Logging")
    print("="*40)
    
    # Test data
    test_input = {
        "longitude": -120.0,
        "latitude": 37.0,
        "housing_median_age": 30.0,
        "total_rooms": 2000.0,
        "total_bedrooms": 400.0,
        "population": 3000.0,
        "households": 500.0,
        "median_income": 7.0,
        "ocean_proximity": "INLAND"
    }
    
    print("ğŸ“Š Input data:")
    for key, value in test_input.items():
        print(f"   {key}: {value}")
    
    # Make prediction
    try:
        response = requests.post("http://localhost:5001/api/predict", json=test_input, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            print(f"\\nâœ… Prediction successful:")
            print(f"   ğŸ  Predicted price: ${result['prediction']:,.2f}")
            print(f"   â±ï¸  Processing time: {result['processing_time_ms']:.2f}ms")
            print(f"   ğŸ¤– Model version: {result['model_version']}")
            
            # Verify MLflow logging
            print(f"\\nğŸ“ MLflow logging:")
            print(f"   âœ… Automatically logged to mlruns_local/")
            print(f"   ğŸ“Š Experiment: api_predictions")
            print(f"   ğŸ•’ Timestamp: {result['timestamp']}")
            
            return True, result['prediction']
        else:
            print(f"âŒ API Error: {response.status_code}")
            print(f"   Response: {response.text}")
            return False, None
            
    except Exception as e:
        print(f"âŒ Request failed: {e}")
        return False, None

def show_recent_predictions():
    """Show recent predictions from MLflow"""
    print("\\nğŸ“ˆ Recent Predictions from MLflow")
    print("="*35)
    
    try:
        import mlflow
        import pandas as pd
        
        # Set tracking URI
        mlflow.set_tracking_uri(f"file://{os.path.abspath('mlruns_local')}")
        
        # Get api_predictions experiment
        experiments = mlflow.search_experiments()
        api_exp = [exp for exp in experiments if exp.name == "api_predictions"]
        
        if api_exp:
            runs = mlflow.search_runs(experiment_ids=[api_exp[0].experiment_id], max_results=5)
            
            if len(runs) > 0:
                print(f"ğŸ“Š Found {len(runs)} recent predictions:")
                
                for i, (_, run) in enumerate(runs.iterrows(), 1):
                    run_id = run['run_id'][:8]
                    start_time = run['start_time'].strftime('%H:%M:%S') if pd.notna(run['start_time']) else 'N/A'
                    prediction = run.get('metrics.prediction_value', 0)
                    processing_time = run.get('metrics.processing_time_ms', 0)
                    
                    print(f"\\n   {i}. Run {run_id} ({start_time})")
                    print(f"      ğŸ  Prediction: ${prediction:,.2f}")
                    print(f"      â±ï¸  Processing: {processing_time:.2f}ms")
                    
                    # Show some input parameters
                    longitude = run.get('metrics.input_longitude')
                    latitude = run.get('metrics.input_latitude')
                    if longitude and latitude:
                        print(f"      ğŸ“ Location: ({longitude}, {latitude})")
            else:
                print("âŒ No predictions found in MLflow")
        else:
            print("âŒ No api_predictions experiment found")
            
    except ImportError:
        print("âŒ MLflow not available (run: conda activate BITSAIML)")
    except Exception as e:
        print(f"âŒ Error reading MLflow data: {e}")

def show_usage_examples():
    """Show usage examples"""
    print("\\nğŸ’¡ MLflow Integration Usage")
    print("="*30)
    
    print("ğŸ“‹ 1. Making predictions (they auto-log to MLflow):")
    print("   curl -X POST -H 'Content-Type: application/json' \\\\")
    print("        -d '{\"longitude\": -122.0, \"latitude\": 37.0, ...}' \\\\") 
    print("        http://localhost:5001/api/predict")
    
    print("\\nğŸ“‹ 2. Direct MLflow logging in Python:")
    print("   from mlflow_integration import log_prediction_to_mlflow")
    print("   log_prediction_to_mlflow(input_data, prediction, processing_time)")
    
    print("\\nğŸ“‹ 3. View MLflow UI:")
    print("   conda activate BITSAIML")
    print(f"   mlflow ui --backend-store-uri file://{os.path.abspath('mlruns_local')}")
    print("   # Then open: http://localhost:5000")
    
    print("\\nğŸ“‹ 4. Access data programmatically:")
    print("   import mlflow")
    print(f"   mlflow.set_tracking_uri('file://{os.path.abspath('mlruns_local')}')")
    print("   runs = mlflow.search_runs()")

def main():
    """Main function"""
    print("ğŸ¯ MLflow Integration - Quick Start Guide")
    print("="*45)
    print("   Simple, Local, File-based MLflow Tracking")
    print("   No server dependencies, always works!")
    print()
    
    # Show status
    show_status()
    
    # Test prediction
    success, prediction = test_prediction_with_logging()
    
    # Show recent predictions
    show_recent_predictions()
    
    # Show usage
    show_usage_examples()
    
    # Summary
    print("\\nğŸ‰ Summary:")
    if success:
        print("   âœ… MLflow integration working perfectly!")
        print(f"   ğŸ  Latest prediction: ${prediction:,.2f}")
        print("   ğŸ“Š All predictions automatically logged")
        print("   ğŸ’¾ Data stored locally in mlruns_local/")
        print("   ğŸŒ View in MLflow UI when needed")
    else:
        print("   âš ï¸  API not responding - check if server is running:")
        print("   conda activate BITSAIML")
        print("   cd /Volumes/Development/Development/assignment/MLOps_ASS1")
        print("   python -m src.api.app")

if __name__ == "__main__":
    main()
